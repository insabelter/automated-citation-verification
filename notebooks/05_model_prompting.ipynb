{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting the models to classify the statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 3.5 and GPT 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "%run ./04_prompt_creation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/dfs/only_text_1024_20/ReferenceErrorDetection_data_with_chunk_info.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunking = \"1024_20\"\n",
    "only_text = True\n",
    "ai_prompt = False\n",
    "suit_prompt = True\n",
    "\n",
    "path = f\"../data/dfs/{'only_text_' if only_text else ''}{chunking}/ReferenceErrorDetection_data_with_chunk_info.pkl\"\n",
    "print(path)\n",
    "\n",
    "# read the dataframe from a pickle file\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Citing Article ID</th>\n",
       "      <th>Citing Article DOI</th>\n",
       "      <th>Citing Article Title</th>\n",
       "      <th>Citing Article Retracted</th>\n",
       "      <th>Citing Article Downloaded</th>\n",
       "      <th>Domain</th>\n",
       "      <th>Citation ID</th>\n",
       "      <th>Statement with Citation</th>\n",
       "      <th>Corrected Statement</th>\n",
       "      <th>...</th>\n",
       "      <th>Reference Article PDF Available</th>\n",
       "      <th>Reference Article Retracted</th>\n",
       "      <th>Reference Article Downloaded</th>\n",
       "      <th>Label</th>\n",
       "      <th>Explanation</th>\n",
       "      <th>Error Type</th>\n",
       "      <th>Added</th>\n",
       "      <th>Previously Partially Substantiated</th>\n",
       "      <th>Top_3_Chunk_IDs</th>\n",
       "      <th>Top_3_Chunk_Texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PubPeer</td>\n",
       "      <td>c001</td>\n",
       "      <td>10.1016/j.est.2021.103553</td>\n",
       "      <td>Heating a residential building using the heat ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>cit001_1</td>\n",
       "      <td>Others have aimed to reduce irreversibility or...</td>\n",
       "      <td>Others have aimed to reduce irreversibility or...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unsubstantiated</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2d702f6c-a1b1-4e0d-bc5b-efc4a257f7e3, 55221b8...</td>\n",
       "      <td>[-en, maintenance personnels can check the mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PubPeer</td>\n",
       "      <td>c001</td>\n",
       "      <td>10.1016/j.est.2021.103553</td>\n",
       "      <td>Heating a residential building using the heat ...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>cit001_2</td>\n",
       "      <td>Some researchers have also studied various hea...</td>\n",
       "      <td>Some researchers have also studied various hea...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unsubstantiated</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[118c524c-3f14-4fe2-8d06-ce6d6570e788, d5cf0f4...</td>\n",
       "      <td>[Introduction\\nThe mixture composed of nanopar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PubPeer</td>\n",
       "      <td>c002</td>\n",
       "      <td>10.1155/2022/4601350</td>\n",
       "      <td>Oxidative Potential and Nanoantioxidant Activi...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Chemistry</td>\n",
       "      <td>cit002_1</td>\n",
       "      <td>The relative content of total flavonoids in th...</td>\n",
       "      <td>The relative content of total flavonoids in th...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unsubstantiated</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[cb961e2a-5ede-4186-b1ec-33297d140cdd, 9d6cf7b...</td>\n",
       "      <td>[This is the simple industrial flow. The strip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PubPeer</td>\n",
       "      <td>c003</td>\n",
       "      <td>10.1155/2022/2408685</td>\n",
       "      <td>The Choice of Anesthetic Drugs in Outpatient H...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Medicine</td>\n",
       "      <td>cit003_1</td>\n",
       "      <td>Research has shown that remimazolam tosylate e...</td>\n",
       "      <td>Research has shown that remimazolam tosylate e...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unsubstantiated</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[d18a377e-c8dc-47ab-988f-0f8655f1fdc4, cd058e6...</td>\n",
       "      <td>[Low perioperative levels of NK activity are a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PubPeer</td>\n",
       "      <td>c004</td>\n",
       "      <td>10.1155/2022/4783847</td>\n",
       "      <td>A Fault-Tolerant Structure for Nano-Power Comm...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>cit004_1</td>\n",
       "      <td>if the efficiency of the routing algorithm is ...</td>\n",
       "      <td>If the efficiency of the routing algorithm is ...</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Unsubstantiated</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[dba1e500-ca49-4b33-8d1d-106beafbf1b3, a230068...</td>\n",
       "      <td>[As can be seen from the figure, the most freq...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Source Citing Article ID         Citing Article DOI  \\\n",
       "0  PubPeer              c001  10.1016/j.est.2021.103553   \n",
       "1  PubPeer              c001  10.1016/j.est.2021.103553   \n",
       "2  PubPeer              c002       10.1155/2022/4601350   \n",
       "3  PubPeer              c003       10.1155/2022/2408685   \n",
       "4  PubPeer              c004       10.1155/2022/4783847   \n",
       "\n",
       "                                Citing Article Title Citing Article Retracted  \\\n",
       "0  Heating a residential building using the heat ...                      Yes   \n",
       "1  Heating a residential building using the heat ...                      Yes   \n",
       "2  Oxidative Potential and Nanoantioxidant Activi...                      Yes   \n",
       "3  The Choice of Anesthetic Drugs in Outpatient H...                      Yes   \n",
       "4  A Fault-Tolerant Structure for Nano-Power Comm...                      Yes   \n",
       "\n",
       "  Citing Article Downloaded       Domain Citation ID  \\\n",
       "0                       Yes  Engineering    cit001_1   \n",
       "1                       Yes  Engineering    cit001_2   \n",
       "2                       Yes    Chemistry    cit002_1   \n",
       "3                       Yes     Medicine    cit003_1   \n",
       "4                       Yes  Engineering    cit004_1   \n",
       "\n",
       "                             Statement with Citation  \\\n",
       "0  Others have aimed to reduce irreversibility or...   \n",
       "1  Some researchers have also studied various hea...   \n",
       "2  The relative content of total flavonoids in th...   \n",
       "3  Research has shown that remimazolam tosylate e...   \n",
       "4  if the efficiency of the routing algorithm is ...   \n",
       "\n",
       "                                 Corrected Statement  ...  \\\n",
       "0  Others have aimed to reduce irreversibility or...  ...   \n",
       "1  Some researchers have also studied various hea...  ...   \n",
       "2  The relative content of total flavonoids in th...  ...   \n",
       "3  Research has shown that remimazolam tosylate e...  ...   \n",
       "4  If the efficiency of the routing algorithm is ...  ...   \n",
       "\n",
       "  Reference Article PDF Available Reference Article Retracted  \\\n",
       "0                             Yes                          No   \n",
       "1                             Yes                          No   \n",
       "2                             Yes                          No   \n",
       "3                             Yes                          No   \n",
       "4                             Yes                          No   \n",
       "\n",
       "  Reference Article Downloaded            Label  Explanation  Error Type  \\\n",
       "0                          Yes  Unsubstantiated   Irrelevant  Irrelevant   \n",
       "1                          Yes  Unsubstantiated   Irrelevant  Irrelevant   \n",
       "2                          Yes  Unsubstantiated   Irrelevant  Irrelevant   \n",
       "3                          Yes  Unsubstantiated   Irrelevant  Irrelevant   \n",
       "4                          Yes  Unsubstantiated   Irrelevant  Irrelevant   \n",
       "\n",
       "  Added Previously Partially Substantiated  \\\n",
       "0    No                                NaN   \n",
       "1    No                                NaN   \n",
       "2    No                                NaN   \n",
       "3    No                                NaN   \n",
       "4    No                                NaN   \n",
       "\n",
       "                                     Top_3_Chunk_IDs  \\\n",
       "0  [2d702f6c-a1b1-4e0d-bc5b-efc4a257f7e3, 55221b8...   \n",
       "1  [118c524c-3f14-4fe2-8d06-ce6d6570e788, d5cf0f4...   \n",
       "2  [cb961e2a-5ede-4186-b1ec-33297d140cdd, 9d6cf7b...   \n",
       "3  [d18a377e-c8dc-47ab-988f-0f8655f1fdc4, cd058e6...   \n",
       "4  [dba1e500-ca49-4b33-8d1d-106beafbf1b3, a230068...   \n",
       "\n",
       "                                   Top_3_Chunk_Texts  \n",
       "0  [-en, maintenance personnels can check the mot...  \n",
       "1  [Introduction\\nThe mixture composed of nanopar...  \n",
       "2  [This is the simple industrial flow. The strip...  \n",
       "3  [Low perioperative levels of NK activity are a...  \n",
       "4  [As can be seen from the figure, the most freq...  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting the models (batch processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "prompt_chars = []\n",
    "\n",
    "def create_batch_files(df, model, number_files=1, ignore_ids=[], ai_prompt=False, suit_prompt=False):\n",
    "    global prompt_chars\n",
    "\n",
    "    output_dir = f\"../data/batch_files/{'only_text_' if only_text else ''}{chunking}/{model}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}\"\n",
    "    # Empty the folder if it exists\n",
    "    if os.path.exists(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    output_files = []\n",
    "    for i in range(number_files):\n",
    "        output_file = os.path.join(output_dir, f\"prompt_batch_{i}.jsonl\")\n",
    "        # If the file already exists, empty it\n",
    "        open(output_file, \"w\").close()\n",
    "        output_files.append(output_file)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if row['Reference Article Downloaded'] == 'Yes' and index not in ignore_ids:\n",
    "            if ai_prompt:\n",
    "                prompt = create_prompt_ai_improved(row)\n",
    "            elif suit_prompt:\n",
    "                prompt = create_prompt_suitability(row)\n",
    "            else:\n",
    "                prompt = create_prompt(row)\n",
    "\n",
    "            prompt_char = len(prompt)\n",
    "            prompt_chars.append(prompt_char)\n",
    "\n",
    "            json_sequence = {\n",
    "                \"custom_id\": f\"request-{index}\", \n",
    "                \"method\": \"POST\", \n",
    "                \"url\": \"/v1/chat/completions\", \n",
    "                \"body\": {\n",
    "                    \"model\": model, \n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt\n",
    "                        }\n",
    "                    ],\n",
    "                    \"temperature\": 0,\n",
    "                }\n",
    "            }\n",
    "\n",
    "            output_file = output_files[index % number_files]\n",
    "            with open(output_file, \"a\") as f:\n",
    "                f.write(json.dumps(json_sequence) + \"\\n\")\n",
    "                \n",
    "    # Remove empty output files from list\n",
    "    output_files = [file for file in output_files if os.path.getsize(file) > 0]\n",
    "    \n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "models = [\"gpt-3.5-turbo-0125\", \"gpt-4.1-nano-2025-04-14\", \"gpt-4.1-mini-2025-04-14\", \"gpt-4.1-2025-04-14\"]\n",
    "model = models[3]\n",
    "\n",
    "os.makedirs(f\"../data/batch_responses/{'only_text_' if only_text else ''}{chunking}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}\", exist_ok=True)\n",
    "responses_dict_path = f\"../data/batch_responses/{'only_text_' if only_text else ''}{chunking}{'/AI_prompt/' if ai_prompt else ''}{'/suit_prompt/' if suit_prompt else ''}/{model}_responses_dict_batch.json\"\n",
    "\n",
    "responses_dict = {}\n",
    "try:\n",
    "    with open(responses_dict_path, 'r') as file:\n",
    "        responses_dict = json.load(file)\n",
    "    ids_to_ignore = [int(key) for key in responses_dict.keys()]\n",
    "except FileNotFoundError:\n",
    "    ids_to_ignore = []\n",
    "\n",
    "print(ids_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/batch_files/only_text_1024_20/gpt-4.1-2025-04-14/suit_prompt/prompt_batch_0.jsonl',\n",
       " '../data/batch_files/only_text_1024_20/gpt-4.1-2025-04-14/suit_prompt/prompt_batch_1.jsonl',\n",
       " '../data/batch_files/only_text_1024_20/gpt-4.1-2025-04-14/suit_prompt/prompt_batch_2.jsonl',\n",
       " '../data/batch_files/only_text_1024_20/gpt-4.1-2025-04-14/suit_prompt/prompt_batch_3.jsonl',\n",
       " '../data/batch_files/only_text_1024_20/gpt-4.1-2025-04-14/suit_prompt/prompt_batch_4.jsonl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_file_paths = create_batch_files(df, model, 5, ids_to_ignore, ai_prompt=ai_prompt, suit_prompt=suit_prompt)\n",
    "batch_file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4614, 21384, 15754.493927125506)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_chars.sort()\n",
    "min(prompt_chars), max(prompt_chars), sum(prompt_chars)/len(prompt_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the content of open_ai_key.txt into a variable\n",
    "with open('../open_ai_key.txt', 'r') as file:\n",
    "    open_ai_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_batch(batch_id, client):\n",
    "    batch = client.batches.retrieve(batch_id)\n",
    "    print(f\"{batch_id} - Current status: {batch.status}\")\n",
    "    if (batch.status == 'in_progress'):\n",
    "        print(f\"{batch.request_counts.completed} / {batch.request_counts.total} completed\")\n",
    "\n",
    "    if batch.status == 'completed' or batch.status == 'failed':\n",
    "        return batch\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "batch_input_files = []\n",
    "batch_creation_responses = []\n",
    "batches = []\n",
    "\n",
    "client = OpenAI(api_key=open_ai_key)\n",
    "\n",
    "def prompt_model_in_batches():\n",
    "    global batch_input_files\n",
    "    global batch_creation_responses\n",
    "    global batches\n",
    "\n",
    "    for batch_file_path in batch_file_paths:\n",
    "        # Creating input file\n",
    "        if os.stat(batch_file_path).st_size == 0:\n",
    "            print(f\"Skipping empty file: {batch_file_path}\")\n",
    "            continue\n",
    "        batch_input_file = client.files.create(\n",
    "            file=open(batch_file_path, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        print(batch_input_file)\n",
    "        batch_input_files.append(batch_input_file)\n",
    "\n",
    "        # Starting batch job\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "        batch_creation_response = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\"\n",
    "        )\n",
    "        print(\"Started: \" + batch_creation_response.id)\n",
    "\n",
    "        time.sleep(5)\n",
    "        # Check the status of the created batch until it is completed\n",
    "        while True:\n",
    "            batch_id = batch_creation_response.id\n",
    "            batch = check_batch(batch_id, client)\n",
    "            if batch:\n",
    "                if batch.status == \"failed\":\n",
    "                    return\n",
    "                elif batch.status == \"completed\":\n",
    "                    batches.append(batch)\n",
    "                    break\n",
    "            time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT 3.5 Turbo:\n",
    "- 256_20 only_text: 27:22 min\n",
    "- 1024_20: 10:10 \n",
    "- 1024_20 only_text: 34 min (and batch 8 and 9 not successfull due to token limit)\n",
    "\n",
    "GPT 4.1 Nano:\n",
    "- 256_20: 35:10 min\n",
    "- 256_20 only_text: 22:40 min\n",
    "- 1024_20: 15:55 min\n",
    "\n",
    "GPT 4.1 Mini:\n",
    "- 256_20: 19:50 min\n",
    "- 256_20 only_text: 21:40 min\n",
    "- 1024_20: 28:26 min\n",
    "- 1024_20 only_text: 28:20 min (and batch 8 and 9 not successfull due to token limit)\n",
    "\n",
    "GPT 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-HiDs61ARanxWXE116mcake', bytes=3995614, created_at=1760542533, filename='prompt_batch_0.jsonl', object='file', purpose='batch', status='processed', expires_at=1763134533, status_details=None)\n",
      "Started: batch_68efbf46c92c819080a5760f7db4b031\n",
      "batch_68efbf46c92c819080a5760f7db4b031 - Current status: validating\n",
      "batch_68efbf46c92c819080a5760f7db4b031 - Current status: failed\n",
      "CPU times: user 80.9 ms, sys: 16 ms, total: 96.9 ms\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt_model_in_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check all open batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=open_ai_key)\n",
    "\n",
    "current_millis = int(time.time())\n",
    "recently = current_millis - 24 * 60 * 60\n",
    "\n",
    "open_batches = client.batches.list()\n",
    "relevant_open_batches = [batch for batch in open_batches if batch.created_at >= recently]\n",
    "in_progress_batch_ids = [batch.id for batch in relevant_open_batches if batch.status == 'in_progress']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batch_68efa5f819d4819081b526560a95616b']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_progress_batch_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Batch(id='batch_68efbf46c92c819080a5760f7db4b031', completion_window='24h', created_at=1760542534, endpoint='/v1/chat/completions', input_file_id='file-HiDs61ARanxWXE116mcake', object='batch', status='failed', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=Errors(data=[BatchError(code='token_limit_exceeded', line=None, message='Enqueued token limit reached for gpt-4.1-2025-04-14 in organization org-6SCiN9rjR6tU38WJ0DavgNRs. Limit: 900,000 enqueued tokens. Please try again once some in_progress batches have been completed.', param=None)], object='list'), expired_at=None, expires_at=1760628934, failed_at=1760542596, finalizing_at=None, in_progress_at=None, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0), model='gpt-4.1-2025-04-14', usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}}),\n",
       " Batch(id='batch_68efa5f819d4819081b526560a95616b', completion_window='24h', created_at=1760536056, endpoint='/v1/chat/completions', input_file_id='file-GmK27qg9sG4NdHCfCVqJNL', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1760622456, failed_at=None, finalizing_at=None, in_progress_at=1760536118, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=36, failed=0, total=49), model='gpt-4.1-2025-04-14', usage={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_tokens_details': {'cached_tokens': 0}, 'output_tokens_details': {'reasoning_tokens': 0}})]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(relevant_open_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a non-empty value for `file_id` but received None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelevant_open_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_file_id\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/resources/files.py:284\u001b[0m, in \u001b[0;36mFiles.content\u001b[0;34m(self, file_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03mReturns the contents of the specified file.\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_id:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `file_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m extra_headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapplication/binary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(extra_headers \u001b[38;5;129;01mor\u001b[39;00m {})}\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/files/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/content\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    288\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39m_legacy_response\u001b[38;5;241m.\u001b[39mHttpxBinaryResponseContent,\n\u001b[1;32m    292\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a non-empty value for `file_id` but received None"
     ]
    }
   ],
   "source": [
    "client.files.content(relevant_open_batches[0].output_file_id).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConflictError",
     "evalue": "Error code: 409 - {'error': {'message': \"Cannot cancel a batch with status 'completed'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConflictError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcancel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_67e3cf592eb081908cd64e5e1dc55fa0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/resources/batches.py:229\u001b[0m, in \u001b[0;36mBatches.cancel\u001b[0;34m(self, batch_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch_id:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `batch_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/batches/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/cancel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1241\u001b[0m     )\n\u001b[0;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1008\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1007\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1057\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_thesis/citation-verification/.venv/lib/python3.10/site-packages/openai/_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1032\u001b[0m )\n",
      "\u001b[0;31mConflictError\u001b[0m: Error code: 409 - {'error': {'message': \"Cannot cancel a batch with status 'completed'.\", 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "# client.batches.cancel(\"batch_67e3cf592eb081908cd64e5e1dc55fa0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the batch status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def wait_for_batch_completion(batch_id, client, interval=10):\n",
    "    while True:\n",
    "        batch = check_batch(batch_id, client)\n",
    "        if batch != None:\n",
    "            return batch\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_68efa5f819d4819081b526560a95616b - Current status: in_progress\n",
      "36 / 49 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_batch_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch_68efa5f819d4819081b526560a95616b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mwait_for_batch_completion\u001b[0;34m(batch_id, client, interval)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch = wait_for_batch_completion(\"batch_68efa5f819d4819081b526560a95616b\", client, interval=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_68efa5f819d4819081b526560a95616b - Current status: in_progress\n"
     ]
    }
   ],
   "source": [
    "batch = check_batch(\"batch_68efa5f819d4819081b526560a95616b\", client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_68678417bf4c819085e0d77c012d5a7a', completion_window='24h', created_at=1751614487, endpoint='/v1/chat/completions', input_file_id='file-TubVrLSjHxewNncTaHNShi', object='batch', status='completed', cancelled_at=None, cancelling_at=None, completed_at=1751614569, error_file_id='file-39tX7oztfkdnqiNEpYDUMb', errors=None, expired_at=None, expires_at=1751700887, failed_at=None, finalizing_at=1751614556, in_progress_at=1751614490, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=25, total=25))\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save responds of completed batches\n",
    "for batch in batches:\n",
    "    if batch.status != \"completed\":\n",
    "        continue\n",
    "    model_responses = client.files.content(batch.output_file_id).text\n",
    "\n",
    "    # Parse the model_responses into a list of objects\n",
    "    responses_list = [json.loads(line) for line in model_responses.splitlines()]\n",
    "    # print(responses_list)\n",
    "\n",
    "    try:\n",
    "        for response in responses_list:\n",
    "            responses_dict[int(response['custom_id'].split('-')[1])] = response\n",
    "            responses_dict = dict(sorted(responses_dict.items(), key=lambda item: int(item[0])))\n",
    "    except NameError:\n",
    "        responses_dict = {int(response['custom_id'].split('-')[1]): response for response in responses_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save responses_dict to a JSON file\n",
    "with open(responses_dict_path, 'w') as file:\n",
    "    json.dump(responses_dict, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save responds to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column in the dataframe to store the responses\n",
    "if 'Model Classification' not in df.columns:\n",
    "    df['Model Classification'] = None\n",
    "\n",
    "# Iterate through the dataframe\n",
    "for index, row in df.iterrows():\n",
    "    if row['Reference Article Downloaded'] == 'Yes':\n",
    "        i = index\n",
    "        if i not in responses_dict:\n",
    "            i = str(i)\n",
    "        model_response = responses_dict[i]['response']['body']['choices'][0]['message']['content']\n",
    "        \n",
    "        # Save the response to the new column\n",
    "        df.at[index, 'Model Classification'] = model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_path = f\"../data/dfs/{'only_text_' if only_text else ''}{chunking}/{model}/{'AI_prompt/' if ai_prompt else ''}\"\n",
    "os.makedirs(dfs_path, exist_ok=True)\n",
    "df.to_pickle(f\"{dfs_path}ReferenceErrorDetection_data_with_prompt_results.pkl\")\n",
    "df.to_excel(f\"{dfs_path}ReferenceErrorDetection_data_with_prompt_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
